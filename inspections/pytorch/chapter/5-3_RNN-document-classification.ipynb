{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNと文章のクラス分類\n",
    "\n",
    "この節では文章のクラス分類を扱います。\n",
    "\n",
    "例えばニュースのジャンル分類やレビュー文章のポジネガ分類などに応用可能です。\n",
    "\n",
    "なお、時系列の分類問題は一般に系列ラベリングと呼ばれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDbレビューデータセット\n",
    "\n",
    "IMDbはアマゾン社で運営されているレビューサイトであり、レビューは0から10までのスコアがつけられます。\n",
    "\n",
    "ここからスタンフォード大学の研究者らが50000件のレビューを抽出し文章のポジネガ分析のベンチマークデータセットとして公開しています。\n",
    "\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imdb.vocabはこのレビューに登場しているすべての単語を事前に抽出したボキャブラリーファイルです。\n",
    "\n",
    "train/posには訓練用のポジティブなレビューのテキストファイルが大量に入っていてほかも同様です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pathlib\n",
    "import re\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import (DataLoader, Dataset, TensorDataset)\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_marks_regex = re.compile(\"[,\\.\\(\\)\\[\\]\\*:;]|<.*?>\")\n",
    "shift_marks_regex = re.compile(\"([?!])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2ids(text, vocab_dict):\n",
    "  text = remove_marks_regex.sub(\"\", text)\n",
    "  text = shift_marks_regex.sub(r\" \\1 \", text)\n",
    "  tokens = text.split()\n",
    "\n",
    "  return [vocab_dict.get(token, 0) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2tensor(token_idxes, max_len=100, padding=True):\n",
    "  if len(token_idxes) > max_len:\n",
    "    token_idxes = token_idxes[:max_len]\n",
    "  n_tokens = len(token_idxes)\n",
    "  if padding:\n",
    "    token_idxes = token_idxes + [0] * (max_len - n_tokens)\n",
    "  return torch.tensor(token_idxes, dtype=torch.int64), n_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text2idsは長い文字列をトークンIDのリストに変換する関数です。\n",
    "\n",
    "list2tensorはIDのリストをint64のTensorに変換する関数です。\n",
    "\n",
    "### Datasetクラスの作成\n",
    "\n",
    "この2つの関数を使用して次のようにDatasetクラスを作ります。\n",
    "\n",
    "コンストラクタ内でテキストファイルのパスとラベルをまとめたTupleのリストをつくり、__getitem__ないでそのファイルを実際に読み取ってTensorに変換しているのがポイントです。\n",
    "\n",
    "Tensorはmax_lenで指定される長さにパディングされて統一されるので、その後の扱いが容易になります。\n",
    "\n",
    "また、0でパディングする前のもともとの長さもn_tokensも後で必要ですのでいっしょに返します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "  def __init__(self, dir_path, train=True,\n",
    "                    max_len=100, padding=True):\n",
    "    self.max_len = max_len\n",
    "    self.padding = padding\n",
    "    \n",
    "    path = pathlib.Path(dir_path)\n",
    "    vocab_path = path.joinpath(\"imdb.vocab\")\n",
    "    \n",
    "    # ボキャブラリファイルを読み込み、行ごとに分割\n",
    "    self.vocab_array = vocab_path.open() \\\n",
    "                        .read().strip().splitlines()\n",
    "    # 単語をキーとし、値がIDのdictを作る\n",
    "    self.vocab_dict = dict((w, i+1) \\\n",
    "        for (i, w) in enumerate(self.vocab_array))\n",
    "    if train:\n",
    "        target_path = path.joinpath(\"train\")\n",
    "    else:\n",
    "        target_path = path.joinpath(\"test\")\n",
    "    pos_files = sorted(glob.glob(\n",
    "        str(target_path.joinpath(\"pos/*.txt\"))))\n",
    "    neg_files = sorted(glob.glob(\n",
    "        str(target_path.joinpath(\"neg/*.txt\"))))\n",
    "    # posは1, negは0のlabelを付けて\n",
    "    # (file_path, label)のtupleのリストを作成\n",
    "    self.labeled_files = \\\n",
    "        list(zip([0]*len(neg_files), neg_files )) + \\\n",
    "        list(zip([1]*len(pos_files), pos_files))\n",
    "  \n",
    "  @property\n",
    "  def vocab_size(self):\n",
    "    return len(self.vocab_array)  \n",
    "  def __len__(self):\n",
    "    return len(self.labeled_files)  \n",
    "  def __getitem__(self, idx):\n",
    "    label, f = self.labeled_files[idx]\n",
    "    # ファイルのテキストデータを読み取って小文字に変換\n",
    "    data = open(f).read().lower()\n",
    "    # テキストデータをIDのリストに変換\n",
    "    data = text2ids(data, self.vocab_dict)\n",
    "    # IDのリストをTensorに変換\n",
    "    data, n_tokens = list2tensor(data, self.max_len, self.padding)\n",
    "    return data, label, n_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練用とテスト用のDataLoaderの作成\n",
    "\n",
    "あとはこれまでの章と同様にこれを利用して訓練用とテスト用のDataLoaderを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = IMDBDataset(\"../data/aclImdb/\")\n",
    "test_data = IMDBDataset(\"../data/aclImdb/\", train=False)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTaggingNet(nn.Module):\n",
    "    def __init__(self, num_embeddings,\n",
    "                 embedding_dim=50, \n",
    "                 hidden_size=50,\n",
    "                 num_layers=1,\n",
    "                 dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim,\n",
    "                                padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, h0=None, l=None):\n",
    "        # IDをEmbeddingで多次元のベクトルに変換する\n",
    "        # xは(batch_size, step_size) \n",
    "        # -> (batch_size, step_size, embedding_dim)\n",
    "        x = self.emb(x)\n",
    "        # 初期状態h0と共にRNNにxを渡す\n",
    "        # xは(batch_size, step_size, embedding_dim)\n",
    "        # -> (batch_size, step_size, hidden_dim)\n",
    "        x, h = self.lstm(x, h0)\n",
    "        # 最後のステップのみ取り出す\n",
    "        # xは(batch_size, step_size, hidden_dim)\n",
    "        # -> (batch_size, 1)\n",
    "        if l is not None:\n",
    "            # 入力のもともとの長さがある場合はそれを使用する\n",
    "            x = x[list(range(len(x))), l-1, :]\n",
    "        else:\n",
    "            # なければ単純に最後を使用する\n",
    "            x = x[:, -1, :]\n",
    "        # 取り出した最後のステップを線形層に入れる\n",
    "        x = self.linear(x)\n",
    "        # 余分な次元を削除する\n",
    "        # (batch_size, 1) -> (batch_size, )\n",
    "        x = x.squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(net, data_loader, device=\"cpu\"):\n",
    "    net.eval()\n",
    "    ys = []\n",
    "    ypreds = []\n",
    "    for x, y, l in data_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        l = l.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = net(x, l=l)\n",
    "            y_pred = (y_pred > 0).long()\n",
    "            ys.append(y)\n",
    "            ypreds.append(y_pred)\n",
    "    ys = torch.cat(ys)\n",
    "    ypreds = torch.cat(ypreds)\n",
    "    acc = (ys == ypreds).float().sum() / len(ys)\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 244.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6837702416398032 0.5338799953460693 0.5325199961662292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 259.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.6856625672344052 0.5612399578094482 0.5541599988937378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 259.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.6801583781419203 0.5688799619674683 0.5623599886894226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 259.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.6504179834938415 0.7139999866485596 0.6867199540138245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 258.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.5841556534056773 0.7683199644088745 0.7126399874687195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 247.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.4834040866597839 0.8215599656105042 0.7480799555778503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 240.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.39776501604510694 0.8581599593162537 0.7680400013923645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:02<00:00, 261.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.3305620495944529 0.8900399804115295 0.7666800022125244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 246.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.28193727903582555 0.8029199838638306 0.7033999562263489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 259.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.2369297416142338 0.9423199892044067 0.7879999876022339\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "# num_embeddingsには0を含めてtrain_data.vocab_size+1を入れる\n",
    "net = SequenceTaggingNet(train_data.vocab_size+1, num_layers=2)\n",
    "net.to(\"cuda:0\")\n",
    "opt = optim.Adam(net.parameters())\n",
    "loss_f = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    net.train()\n",
    "    for x, y, l in tqdm.tqdm(train_loader):\n",
    "        x = x.to(\"cuda:0\")\n",
    "        y = y.to(\"cuda:0\")\n",
    "        l = l.to(\"cuda:0\")\n",
    "        y_pred = net(x, l=l)\n",
    "        loss = loss_f(y_pred, y.float())\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    train_acc = eval_net(net, train_loader, \"cuda:0\")\n",
    "    val_acc = eval_net(net, test_loader, \"cuda:0\")\n",
    "    print(epoch, mean(losses), train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tk/university/zemi/tokai_teio/inspections/pytorch/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.93124, 0.39392)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_X, train_y = load_svmlight_file(\n",
    "    \"../data/aclImdb/train/labeledBow.feat\")\n",
    "test_X, test_y = load_svmlight_file(\n",
    "    \"../data/aclImdb/test/labeledBow.feat\",\n",
    "    n_features=train_X.shape[1])\n",
    "\n",
    "model = LogisticRegression(C=0.1, max_iter=1000)\n",
    "model.fit(train_X, train_y)\n",
    "model.score(train_X, train_y), model.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTaggingNet2(SequenceTaggingNet):\n",
    "\n",
    "    def forward(self, x, h0=None, l=None):\n",
    "        # IDをEmbeddingで多次元のベクトルに変換\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        # 長さ情報が与えられている場合はPackedSequenceを作る\n",
    "        if l is not None:\n",
    "            x = nn.utils.rnn.pack_padded_sequence(\n",
    "                x, l, batch_first=True)\n",
    "        \n",
    "        # RNNに通す\n",
    "        x, h = self.lstm(x, h0)\n",
    "        \n",
    "        # 最後のステップを取り出して線形層に入れる\n",
    "        if l is not None:\n",
    "            # 長さ情報がある場合は最後の層の\n",
    "            # 内部状態のベクトルを直接利用できる\n",
    "            # LSTMは通常の内部状態の他にブロックセルの状態も\n",
    "            # あるので内部状態のみを使用する\n",
    "            hidden_state, cell_state = h\n",
    "            x = hidden_state[-1]\n",
    "        else:\n",
    "            x = x[:, -1, :]\n",
    "            \n",
    "        # 線形層に入れる\n",
    "        x = self.linear(x).squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 243.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.20585377895228013 0.9509599804878235 0.7823999524116516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 244.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.162170249618628 0.9549999833106995 0.7743600010871887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 245.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.13027308418122513 0.976919949054718 0.7804799675941467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 244.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.09978278950237862 0.9821999669075012 0.7774399518966675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 245.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.08148309446113837 0.9873199462890625 0.7752799987792969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 245.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.06693952572067528 0.9870399832725525 0.7712000012397766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 244.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.05861038223261495 0.991159975528717 0.7753199934959412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 245.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.05059656970467313 0.9889199733734131 0.7694399952888489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 245.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.05079126190942 0.9908799529075623 0.770359992980957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:03<00:00, 246.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.05038726589490024 0.9933199882507324 0.7657999992370605\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    net.train()\n",
    "    for x, y, l in tqdm.tqdm(train_loader):\n",
    "        # 長さの配列を長い順にソート\n",
    "        l, sort_idx = torch.sort(l, descending=True)\n",
    "        # 得られたインデクスを使用してx,yも並べ替え\n",
    "        x = x[sort_idx]\n",
    "        y = y[sort_idx]\n",
    "        \n",
    "        x = x.to(\"cuda:0\")\n",
    "        y = y.to(\"cuda:0\")\n",
    "        \n",
    "        y_pred = net(x, l=l)\n",
    "        loss = loss_f(y_pred, y.float())\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    train_acc = eval_net(net, train_loader, \"cuda:0\")\n",
    "    val_acc = eval_net(net, test_loader, \"cuda:0\")\n",
    "    print(epoch, mean(losses), train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b9f220186b1679e4c67686f80c503f2a68b9789d94e8dfa473b7907788bc702"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
